
import tarfile
import os

# Path to the uploaded file
file_path = "//content//aclImdb_v1.tar.gz"
extracted_path = "//mnt//data//aclImdb"

# Attempt to extract the file with error handling
if not os.path.exists(extracted_path):
    try:
        with tarfile.open(file_path, 'r:gz') as tar:
            tar.extractall(path=extracted_path)
        print(f"Successfully extracted to {extracted_path}")
    except tarfile.ReadError as e:
        print(f"Error reading the tar file: {e}")
    except EOFError as e:
        print(f"EOFError: {e} - The file may be corrupted.")
else:
    print("The dataset is already extracted.")


    # Define paths for training and testing data
train_pos = os.path.join(extracted_path, 'aclImdb', 'train', 'pos')
train_neg = os.path.join(extracted_path, 'aclImdb', 'train', 'neg')
test_pos = os.path.join(extracted_path, 'aclImdb', 'test', 'pos')
test_neg = os.path.join(extracted_path, 'aclImdb', 'test', 'neg')

print("Paths for train and test data defined.")


def load_reviews(path, label):
    """
    Reads all files in a directory and assigns a label (1 for positive, 0 for negative).
    Args:
        path (str): Path to the review files.
        label (int): Label to assign to the reviews (1 or 0).
    Returns:
        list: List of dictionaries containing the review text and label.
    """
    reviews = []
    for filename in os.listdir(path):
        with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:
            reviews.append({'review': file.read(), 'label': label})
    return reviews

print("Function to load reviews is ready.")

# Load reviews for training and testing datasets
train_data = load_reviews(train_pos, 1) + load_reviews(train_neg, 0)
test_data = load_reviews(test_pos, 1) + load_reviews(test_neg, 0)

print(f"Loaded {len(train_data)} training reviews and {len(test_data)} testing reviews.")

import pandas as pd

# Convert to DataFrame
train_df = pd.DataFrame(train_data)
test_df = pd.DataFrame(test_data)

# Save as CSV
train_csv_path = "//mnt//data//imdb_train.csv"
test_csv_path = "//mnt//data//imdb_test.csv"

train_df.to_csv(train_csv_path, index=False)
test_df.to_csv(test_csv_path, index=False)

print(f"Training data saved to {train_csv_path}")
print(f"Testing data saved to {test_csv_path}")

import pandas as pd

# Load the training and testing data
train_df = pd.read_csv("//mnt//data//imdb_train.csv")
test_df = pd.read_csv("//mnt//data//imdb_test.csv")

print(f"Training Data: {train_df.shape}")
print(f"Testing Data: {test_df.shape}")


# Ensure all values in the 'review' column are strings
train_df['review'] = train_df['review'].astype(str)
test_df['review'] = test_df['review'].astype(str)

# Apply preprocessing
train_df['review'] = train_df['review'].apply(preprocess_text)
test_df['review'] = test_df['review'].apply(preprocess_text)

print("Text preprocessing complete.")

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the vectorizer
vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform the training data
X_train = vectorizer.fit_transform(train_df['review'])
y_train = train_df['label']

# Transform the test data
X_test = vectorizer.transform(test_df['review'])
y_test = test_df['label']

print("Text vectorization complete.")

from sklearn.linear_model import LogisticRegression

# Initialize and train the model
model = LogisticRegression()
model.fit(X_train, y_train)

print("Model training complete.")

from sklearn.metrics import accuracy_score, classification_report

# Make predictions
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))


import joblib

# Save the model and vectorizer
joblib.dump(model, "//mnt//data//sentiment_model.pkl")
joblib.dump(vectorizer, "//mnt//data//tfidf_vectorizer.pkl")

print("Model and vectorizer saved.")


from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the vectorizer
vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform the training data
X_train = vectorizer.fit_transform(train_df['review'])
y_train = train_df['label']

# Transform the test data
X_test = vectorizer.transform(test_df['review'])
y_test = test_df['label']

print("Text vectorization complete.")

from sklearn.linear_model import LogisticRegression

# Initialize the model
model = LogisticRegression(max_iter=200)

# Train the model
model.fit(X_train, y_train)

print("Model training complete.")

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Make predictions
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Display classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Display confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

import joblib

# Save the model
model_path = "//mnt//data//sentiment_model.pkl"
vectorizer_path = "//mnt//data//tfidf_vectorizer.pkl"

joblib.dump(model, model_path)
joblib.dump(vectorizer, vectorizer_path)

print(f"Model saved to {model_path}")
print(f"Vectorizer saved to {vectorizer_path}")


# Load the saved model and vectorizer
loaded_model = joblib.load(model_path)
loaded_vectorizer = joblib.load(vectorizer_path)

# Predict sentiment for a new review
new_review = "The movie was absolutely fantastic! The acting was brilliant."
preprocessed_review = preprocess_text(new_review)
vectorized_review = loaded_vectorizer.transform([preprocessed_review])

# Make prediction
predicted_label = loaded_model.predict(vectorized_review)
sentiment = "Positive" if predicted_label[0] == 1 else "Negative"

print(f"Review: {new_review}")
print(f"Predicted Sentiment: {sentiment}")


from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'lbfgs'],
    'max_iter': [100, 200, 300]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy', verbose=1)

# Perform the search
grid_search.fit(X_train, y_train)

# Best parameters and performance
best_model = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Accuracy:", grid_search.best_score_)

# Evaluate on test data
test_accuracy = best_model.score(X_test, y_test)
print(f"Test Accuracy of Optimized Model: {test_accuracy * 100:.2f}%")


import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Assuming y_test and y_pred are already defined

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the heatmap for Confusion Matrix
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)  # First plot
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix Heatmap')

# Classification Report as a plot (accuracy, precision, recall, f1-score)
plt.subplot(1, 2, 2)  # Second plot
class_report = classification_report(y_test, y_pred, output_dict=True)
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Classification Report')

plt.tight_layout()
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()



# Get feature importance (weights of features)
feature_names = vectorizer.get_feature_names_out()
coefficients = best_model.coef_[0]

# Sort by importance
sorted_features = sorted(zip(coefficients, feature_names), key=lambda x: x[0], reverse=True)

print("Top Positive Words:")
print(sorted_features[:10])

print("\nTop Negative Words:")
print(sorted_features[-10:])


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Build a simple neural network
model_nn = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.5),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the model
model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model_nn.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model_nn.evaluate(X_test, y_test)
print(f"Neural Network Test Accuracy: {test_acc * 100:.2f}%")


def predict_sentiment(review):
    """
    Predict the sentiment of a given review using the trained model.
    """
    preprocessed_review = preprocess_text(review)
    vectorized_review = vectorizer.transform([preprocessed_review])
    prediction = model.predict(vectorized_review)
    return "Positive" if prediction[0] == 1 else "Negative"

# Example usage
new_review = "This movie was absolutely terrible, I hated it!"
print(f"Review: {new_review}")
print(f"Predicted Sentiment: {predict_sentiment(new_review)}")

# Import necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import make_pipeline

# Sample dataset (replace with your actual dataset)
data = {
    'review': [
        'This movie was fantastic!', 'I loved this movie', 'This movie was horrible', 
        'I hated this movie', 'It was okay, not great', 'Best movie ever', 
        'Worst movie I have ever seen'
    ],
    'sentiment': ['Positive', 'Positive', 'Negative', 'Negative', 'Neutral', 'Positive', 'Negative']
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Data preprocessing: Vectorization and encoding labels
X = df['review']  # Features (reviews)
y = df['sentiment']  # Target (sentiment)

# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a pipeline with CountVectorizer and Naive Bayes classifier
model = make_pipeline(CountVectorizer(), MultinomialNB())

# Train the model
model.fit(X_train, y_train)

# Predict sentiment for a new review
new_review = ["This movie was absolutely terrible, I hated it!"]
predicted_sentiment = model.predict(new_review)

# Output the prediction
print(f"Predicted Sentiment: {predicted_sentiment[0]}")

# Evaluate the model
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the heatmap for Confusion Matrix
plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 1)  # First plot (Confusion Matrix Heatmap)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix Heatmap')

# Classification Report as a plot
plt.subplot(1, 3, 2)  # Second plot (Classification Report Heatmap)
class_report = classification_report(y_test, y_pred, output_dict=True)
sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Classification Report')

# ROC Curve (for multi-class classification, OneVsRest method can be used)
fpr, tpr, thresholds = roc_curve(y_test.apply(lambda x: 1 if x == 'Positive' else 0), y_pred == 'Positive')
roc_auc = auc(fpr, tpr)

plt.subplot(1, 3, 3)  # Third plot (ROC Curve)
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')

# Show all plots
plt.tight_layout()
plt.show()

# Classification Report & Accuracy in Text
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")


# Import necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import make_pipeline

# Sample dataset (replace with your actual dataset)
data = {
    'review': [
        'This movie was fantastic!', 'I loved this movie', 'This movie was horrible', 
        'I hated this movie', 'It was okay, not great', 'Best movie ever', 
        'Worst movie I have ever seen'
    ],
    'sentiment': ['Positive', 'Positive', 'Negative', 'Negative', 'Neutral', 'Positive', 'Negative']
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Data preprocessing: Vectorization and encoding labels
X = df['review']  # Features (reviews)
y = df['sentiment']  # Target (sentiment)

# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a pipeline with CountVectorizer and Naive Bayes classifier
model = make_pipeline(CountVectorizer(), MultinomialNB())

# Train the model
model.fit(X_train, y_train)

# Predict sentiment for a new review (change this to a positive review)
new_review = ["This movie was absolutely amazing, I loved every part of it!"]
predicted_sentiment = model.predict(new_review)

# Output the prediction
print(f"Predicted Sentiment: {predicted_sentiment[0]}")  # Should output 'Positive'

# Evaluate the model
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Create a new figure for different plots with enlarged size
plt.figure(figsize=(20, 16))  # Increase the figure size for all subplots

# 1. Bar Graph of Sentiment Counts
plt.subplot(2, 3, 1)
sns.countplot(x=y_pred, palette='Blues')
plt.title('Sentiment Count Bar Graph', fontsize=16)
plt.xlabel('Sentiment', fontsize=14)
plt.ylabel('Count', fontsize=14)

# 2. Box Plot for Sentiment Distribution (based on word count)
plt.subplot(2, 3, 2)
df['word_count'] = df['review'].apply(lambda x: len(x.split()))
sns.boxplot(x=y, y=df['word_count'], palette='Set3')
plt.title('Box Plot for Sentiment by Word Count', fontsize=16)
plt.xlabel('Sentiment', fontsize=14)
plt.ylabel('Word Count', fontsize=14)

# 3. Bubble Chart for Sentiment vs. Review Length
plt.subplot(2, 3, 3)
df['review_length'] = df['review'].apply(len)
sns.scatterplot(x=df['review_length'], y=df['word_count'], hue=df['sentiment'], size=df['review_length'], sizes=(100, 400), palette='coolwarm', legend=None)
plt.title('Bubble Chart: Sentiment vs. Review Length', fontsize=16)
plt.xlabel('Review Length', fontsize=14)
plt.ylabel('Word Count', fontsize=14)

# 4. Heatmap for Confusion Matrix
plt.subplot(2, 3, 4)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
plt.title('Confusion Matrix Heatmap', fontsize=16)
plt.xlabel('Predicted', fontsize=14)
plt.ylabel('Actual', fontsize=14)

# 5. ROC Curve for Positive Sentiment (Binary Classification)
fpr, tpr, thresholds = roc_curve(y_test.apply(lambda x: 1 if x == 'Positive' else 0), y_pred == 'Positive')
roc_auc = auc(fpr, tpr)

plt.subplot(2, 3, 5)
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('ROC Curve for Positive Sentiment', fontsize=16)
plt.legend(loc='lower right', fontsize=12)

# 6. Classification Report Bar Graph for Precision and Recall
class_report = classification_report(y_test, y_pred, output_dict=True)
class_report_df = pd.DataFrame(class_report).iloc[:-1, :].T

plt.subplot(2, 3, 6)
class_report_df[['precision', 'recall']].plot(kind='bar', figsize=(10, 8), color=['lightblue', 'lightgreen'])
plt.title('Classification Report: Precision and Recall', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Sentiment', fontsize=14)

# Adjust layout and display all the plots
plt.tight_layout()
plt.show()

# Classification Report & Accuracy in Text
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
